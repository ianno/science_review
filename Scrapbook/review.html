<h1 id="sec:introduction">Introduction</h1>
<p>We review the paper in the spirit of promoting reproducibility of research and attempt to replicate the authors’ work. We also discuss other methods to identify anomalies, and present results based on our analysis using Permutation Tests. Permutation tests are consistent with the aim of the paper–providing simple tools to detect anomalies–and validate the results in the paper, leading to the same conclusions.</p>
<p>Before diving into technical details, we make a minor observation: the organization of the paper was not properly introduced. The use of distinct sections for (1) the discussion on data and experiments; (2) their model and related calculations; (3) the application of common tests from the literature; and (4) conclusions, would have been helpful. The review is organized as follows. In section [reproducibility-of-results] we replicate authors’ work and results and discuss weaknesses of their approach. In section [our-analysis], we propose and implement additional tests to consolidate the results. We finally draw our conclusions in section [conclusion].</p>
<h2 id="problem-set-up">Problem Set Up</h2>
<p>The paper begins by voicing a growing concern towards “Scientific fraud and Plagiarism” in the scientific community and is successful in conveying a strong message. The authors present some statistical figures and point out the existence of easy statistical tools to detect fabricated data and ignorance about such tools.</p>
<p>The authors examine datsets from radio-biological experiments. They find that data reported by one of 10 researchers, the “RTS”, is suspicious. They perform three different tests to validate their suspicion and also validate their tests and assumptions by looking at the data obtained from three other sources. Each researcher made two types of triple measurements - colony counts and Coulter counts. The authors suspect that the RTS fabricated data triples to get the mean s/he desired in each triple by setting one observation equal to the desired mean and the other two roughly equidistant above and below that value. This would result in triples that contain the (rounded) mean as one of their values.</p>
<p>The methodological contribution of the paper is “bounds and estimates for the probability that a given set of <span class="math inline"><em>n</em></span> such triples contains <span class="math inline"><em>k</em></span> or more triples which contain their own mean” when each of the <span class="math inline"><em>n</em></span> triples is independent and identically distributed (i.i.d.) Poisson, and triples are independent of each other. (Different triples may have different Poisson means.) For this Poisson model, the chance that the RTS’s data would contain so many triples that include their rounded mean is astronomically low. They also apply more common tests for anomalous data, based on statistics such as the frequency of the terminal digit and the frequency with which the last two digits are equal. However, some of the questions that were slightly untouched upon are discussed below:</p>
<ul>
<li><p>The authors write, “Having observed what appeared to us to be an unusual frequency of triples in RTS’s data containing a value close to their mean, we used R to calculate the mid-ratios for all of the colony data triples that were available to us.” This suggests that the same data–and the same feature of the data–that raised their suspicions about the RTS was the data used to test whether the RTS’s data were anomalous on the basis of that feature. If so, then the nominal <span class="math inline"><em>p</em></span>-values are likely to be misleadingly small.</p></li>
<li><p>Most of the tests assume a model for the observations and compare the RTS’s data to that model. The authors validate the assumptions of the model by comparing it with the data pooled for the other researchers. Pooling the data in this way may hide anomalies in the other researchers’ data. Permutation tests allow the data from each researcher to be compared to the data from the other researchers without positing a generative model for the data. On the other hand, the bulk of the data available is from the RTS. To reject the hypothesis that another researcher’s data looks like a random sample from the pooled data, if it includes the RTS’s data, does not imply s/he is suspicious. Instead, it shows that his/her data is not like that of the RTS. See section [our-analysis] of this review for more discussion.</p></li>
</ul>
<h1 id="reproducibility-of-results">Reproducibility of Results</h1>
<p>This section discusses our efforts to replicate the analyses in the paper. After fine tuning, we were able to replicate most of their results, obtaining similar results in the other cases. Our work is available on [github.com/ianno/stat215a_project1]. We first discuss specifics about the replication and then comment about the tests and methods involved.</p>
<h2 id="mid-ratio-analysis">Mid-Ratio Analysis</h2>
<p>The authors first consider the mid-ratio, which is defined for a triple <span class="math inline">(<em>a</em>, <em>b</em>, <em>c</em>),<em>a</em> &lt; <em>b</em> &lt; <em>c</em></span> as <span class="math inline">$\frac{b-a}{c-a}$</span>, and show that the histogram of RTS’s data concentrates abnormally around the <span class="math inline">0.4 − 0.6</span> range, compared to the data taken by all the other lab members. After tweaking the default histogram function on numpy, we were able to obtain plots similar to the ones reported in Figure (1) of the paper. Two noticeable differences were - (1) we obtain <span class="math inline">44%</span> chance of seeing mid-ratio in <span class="math inline">(0.4, 0.5]</span> interval for RTS, compared to <span class="math inline">50%</span> chance reported in the paper and (2) we used 1360/1361 and 595/595 triples to compute histogram for RTS and the rest respectively, compared to the use of 1343/1361 and 572/595 triples by the authors. We believe the authors did not provide enough information about the methods used to filter data for this section. However, such minor differences did not demand further investigation.</p>
<h2 id="probability-model">Probability Model and Hypothesis Tests</h2>
<p>The authors develop a model to bound the probability of observing <span class="math inline"><em>k</em></span> out of <span class="math inline"><em>n</em></span> triples contain their mean. Each entry in a triple is assumed to be an independent sample from a Poisson distribution with mean <span class="math inline"><em>λ</em></span>. (Different triples may have different means.) The event of observing the rounded mean in such a triple is a Bernoulli random variable (BRV) whose success probability depends on <span class="math inline"><em>λ</em></span>. The authors derive analytical expressions for these success probabilities in Appendix A. Numerical values of these probabilities, for <span class="math inline"><em>λ</em> = {1, …, 25}</span>, are presented in Table 1. We could replicate this table exactly. For large <span class="math inline"><em>λ</em> (&gt;2000)</span>, for which the authors provide only few representative probability values, our implementation suffered from numerical issues.</p>
<p>Using Table 1, the authors determine the success probability for the BRV in two different ways and use it to compute the chance of observing the data. For hypothesis test I (non-parametric) they used the maximum value from Table 1 as an upper bound for all triples, essentially treating all BRVs as i.i.d. Bernoulli(<span class="math inline">0.42</span>). Replicating this was straightforward. For hypothesis test II and III, the authors use maximum likelihood estimate of <span class="math inline"><em>λ</em></span> for each triple to compute the corresponding success probability values, essentially treating each BRV to have a different distribution. The authors address the sum of these BRVs as a “Poisson Binomial Random Variable&quot;. Additionally, for the hypothesis test III, the authors use normal approximation for the Poisson binomial random variables. We could replicate the probability values, up to minor errors, for the colony data. Limitations of our implementation gave inaccurate results for Coulter data. For sanity checks of the results, we used linearly interpolated estimates from the paper (for intermediate <span class="math inline"><em>λ</em></span>) and obtained values similar to those in the paper for these tests. Figure [table2] is the approximate replication of Table 2 from the paper.</p>
<div class="figure">
<img src="images/HT_Stat_values.png" alt="Approximate Replication of Table 2" />
<p class="caption">Approximate Replication of Table 2<span data-label="table2"></span></p>
</div>
<h2 id="digits-analysis">Digits Analysis</h2>
<p>The authors also perform some common tests for fraud detection - <em>terminal digit analysis</em> and <em>pair of equal terminal digits analysis</em>. These tests are based on the assumption that, in general, insignificant digits of a random sample are uniformly distributed.</p>
<h3 id="terminal-digit-analysis">Terminal Digit Analysis</h3>
<p>The first test assumes that the last digit in samples of large numbers (<span class="math inline">&gt;100</span>) should empirically show uniform distribution. Also, some previous works, e.g. <span class="citation"></span>, have shown that fabricated data often fails to show such peculiar property. The authors use the chi-square test for goodness of fit, and get low <span class="math inline"><em>p</em></span>-values for the RTS’s data, and good fits for the data of other researchers. Our results are very similar to theirs, although not identical.</p>
<h3 id="equal-digits-analysis">Equal Digits Analysis</h3>
<p>This test assumes that, for large numbers, empirical frequencies of observations of a pair of equal terminal digits should be close to <span class="math inline">1/10</span>. The authors did not mention which tests were considered for this analysis. We assume they performed chi-square tests for goodness of fit, for which we obtain similar results.</p>
<h2 id="discussion-of-assumptions">Discussion</h2>
<p>Here are a few general comments on the methodology adopted by the authors:</p>
<ul>
<li><p>The authors did not justify the assumption of Poisson distribution for the underlying radio-biological data. We think a more thorough explanation would have been helpful for readers with different backgrounds.</p></li>
<li><p>The authors suspected RTS’s data, but used his/her data to fit a model and quantify their suspicion. While sometimes this may raise concerns, here we agree with the authors that doing so increases the odds in favor of the RTS, hence giving us desirable conservative results.</p></li>
<li><p>The authors do not discuss why considering only numbers larger than 100 justifies the assumption of insignificance for the two terminal digits.</p></li>
<li><p>The authors include additional data from three external sources (two for Coulter counts and one for colony counts). All of them, however, had a relatively small amount of data. Despite the authors’ attempts to account for this, we believe that in the current setting these additional samples do not provide more compelling evidence. Instead, they might be misleading (Are the procedures used the same? Is the equipment calibrated in the same way?, etc.).</p></li>
<li><p>We reiterate that pooling the data may hide anomalies in the other researchers’ data.</p></li>
</ul>
<h1 id="our-analysis">Further Analysis</h1>
<p>As a preliminary test for identifying suspicious datasets, we (1) plot histograms of mid-ratios for the colony data provided by individual researchers, and (2) contrast the histogram of each investigator with the histogram of the pooled data of the other investigators. Here, we only include plots for (1).</p>
<div class="figure">
<img src="images/new_mid_ratio.png" alt="Individual Histograms for the Colony Data" />
<p class="caption">Individual Histograms for the Colony Data<span data-label="ind_mid_ratio"></span></p>
</div>
<p>Two important observations can be made:</p>
<ul>
<li><p>The histograms for researchers with labels B, C, E, F, G, H, I do not appear following uniform distribution.</p></li>
<li><p>RTS heavily influences the histogram when his/her data is collected in the pool and, therefore, patterns from the other researchers look anomalous when compared to it.</p></li>
</ul>
<p>These points illustrate the limitations of the uniformity assumption for mid-ratios and the visual comparison between the histograms of RTS and the pool to motivate suspicion.</p>
<h2 id="quick-primer-to-permutation-tests">Permutation Tests</h2>
<p>“The problem of determining whether a treatment has an effect is widespread in various real world problems. To evaluate whether a treatment has an effect, it is crucial to compare the outcome when treatment is applied (the outcome for the treatment group) with the outcome when treatment is withheld (the outcome for the control group), in situations that are as alike as possible but for the treatment. This is called the method of comparison.”<span class="citation"></span>. We will describe this method for a specific set up relevant for this review.</p>
<p>Suppose that we are given two sets of observations - one of them labeled as ‘treatment’ with size <span class="math inline"><em>T</em></span>, and the other labeled as ‘control’, of size <span class="math inline"><em>C</em></span>. We assume that the first of them has received a treatment and we wish to test the hypothesis whether this treatment affects the group. In a two-sample permutation test, the data is pooled together to form a population of size <span class="math inline"><em>N</em> = <em>T</em> + <em>C</em></span>. To compare the two groups, we need to decide on a test-statistic that can capture the effect of the treatment (if any) on the population. As an example, we can consider the absolute difference between the sample means of the two datasets. Under the null hypothesis that the treatment has no effect, one can analytically derive the distribution of this test statistic. However, it is often easier to empirically approximate such distribution rather than compute it numerically. To do so, one needs to repeatedly randomly partition the data into groups of size <span class="math inline"><em>T</em></span> and <span class="math inline"><em>C</em></span> and compute the test statistic contrasting the two groups. We use the empirical histogram obtained from these experiments, as a proxy for the true distribution of the test statistic. Just like typical hypothesis testing, we then determine the chances (<span class="math inline"><em>p</em></span>-value) of observing the test statistic that we computed in the beginning.</p>
<p>When the <span class="math inline"><em>p</em></span>-value is below a preset significance level, we infer that the treatment has an effect at that level of significance. It is unlikely that the two sets were obtained by a random partition of the pooled data.</p>
<h3 id="permutation-tests-for-mid-ratio">Results for Mid-Ratio</h3>
<p>We set the test statistic to be the difference in standard deviation of the mid-ratios for the two datasets. We choose the standard deviation, instead of the mean, because our null and alternative hypothesis for mid-ratio (uniform distribution versus concentration around 0.5) have the same mean (<span class="math inline">0.5</span>). We expect the standard deviation to capture the <em>unintentional reduction in spread caused in data due to intentional adjustments</em>.</p>
<p>We consider each researcher’s data equivalent to a treatment group and the rest of them as the control group. We use 1000 repetitions to obtain the empirical distribution and then compute the <span class="math inline"><em>p</em></span>-values:</p>
<ul>
<li><p><span class="math inline">0.00</span>, for investigators A, B, D, and RTS;</p></li>
<li><p><span class="math inline">&lt;0.01</span>, for C, H, I;</p></li>
<li><p><span class="math inline">&gt;0.01</span>, for E,F,G.</p></li>
</ul>
<p>The <span class="math inline"><em>p</em></span>-values indicate that almost all datasets are surprising with respect to this test-statistic. We would like to emphasize that here a <span class="math inline"><em>p</em></span>-value of <span class="math inline">0.00</span> in fact denotes a <span class="math inline"><em>p</em></span>-value <span class="math inline">&lt;0.001</span>, because of the finite resolution owing to <span class="math inline">1000</span> tests. We would also like to mention that RTS is still the most surprising if one looks at the location of the test-statistic in the tails of the distribution.</p>
<p>We also use <span class="math inline">ℓ<sub>1</sub></span>-distance between the density<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>, and the <span class="math inline">ℓ<sub>1</sub></span>-distance between the cumulative distribution function (CDF) as the test statistic. Again, we reject several researchers of the lab at a significance level of <span class="math inline">1%</span>. We present all the <span class="math inline"><em>p</em></span>-values in Figure [mid_ratio_perm].</p>
<div class="figure">
<img src="images/mid_ratio_perm.png" alt="Results for Permutation Tests for Mid-Ratio" />
<p class="caption">Results for Permutation Tests for Mid-Ratio<span data-label="mid_ratio_perm"></span></p>
</div>
<p><span><strong>Remark</strong></span> We would like to mention that when RTS is included in the control group, it constitutes the bulk of the group. As a result, rejecting the null hypothesis for a researcher is almost equivalent to rejecting the hypothesis that the data of that researcher is same as RTS’s data. If we already believed or discovered that RTS’s data was suspicious, then we cannot flag other researchers’ data as suspicious. Therefore, we do another set of permutation tests after excluding the RTS’s data. We did not find strong evidence to reject the null hypothesis, hence we conclude that none of the researchers is suspicious at a significance level of <span class="math inline">1%</span>. However, this set of tests suffer from a bias because of our manual throwing away 2/3 of the data points.</p>
<div class="figure">
<img src="images/mid_ratio_perm_no_rts.png" alt="Results for Permutation Tests without RTS for Mid Ratios" />
<p class="caption">Results for Permutation Tests without RTS for Mid Ratios</p>
</div>
<p>Putting together all the pieces, we conclude that there is statistical evidence to claim that RTS’s data is not genuine.</p>
<h2 id="additional-tests-for-digit-analysis">Additional Tests for Digit Analysis</h2>
<p>For the terminal digit and equal digits analyses, we extended the tests done by the authors to individual members of the lab and performed (1) chi-square test for goodness of fit for terminal digit, (2) chi-square test for goodness of fit for equal digits and (3) permutation tests for terminal digit. For permutation tests, we used the test statistics listed in the previous section. Results are tabulated in Figures [cst1], [cst2], and [perm2].</p>
<div class="figure">
<img src="images/raaz_term_chi_summary.png" alt="Chi Square Tests for Terminal Digits in Coulter and Colony Counts" />
<p class="caption">Chi Square Tests for Terminal Digits in Coulter and Colony Counts<span data-label="cst1"></span></p>
</div>
<div class="figure">
<img src="images/raaz_eq_chi_elaborate.png" alt="Chi Square Tests for Equal Terminal Pair in Coulter and Colony Counts" />
<p class="caption">Chi Square Tests for Equal Terminal Pair in Coulter and Colony Counts<span data-label="cst2"></span></p>
</div>
<div class="figure">
<img src="images/raaz_eq_perm_summary.png" alt="Permutation Tests for Terminal Digit Analysis, Coulter Counts" />
<p class="caption">Permutation Tests for Terminal Digit Analysis, Coulter Counts<span data-label="perm2"></span></p>
</div>
<p>Figure [perm2], once again, confirms that RTS’s data is suspicious. As before, the huge fraction of data by RTS contributes towards the low <span class="math inline"><em>p</em></span>-values for some of the other researchers. In permutation tests after excluding RTS, none of the researchers look suspicious. For sake of brevity, we avoid mentioning the <span class="math inline"><em>p</em></span>-values here.</p>
<h1 id="conclusion">Conclusion</h1>
<p>Data fraud is an extremely critical issue in science, engineering, and many other fields. Methods to detect manipulated data are needed to identify fraudulent research behaviors. Detecting frauds, however, is a delicate matter. Challenging the credibility of a researcher or of a scientific work, in fact, can have heavy consequences for all the parties involved in the process. Methodologies and techniques used in this kind of work need to be clear and widely accepted. They need to produce results which leave minimal (ideally no) space to ambiguity. Independently, reproducibility of results is a fundamental element to rule out any doubts that could arise at any time. In our review, we carefully analyzed the authors’ work by reproducing the results in the paper and using additional tests which we believe to be more general. We found that authors’ conclusions are correct, having been able to reproduce most of their results. Moreover, we encourage the use of more powerful tools, such as permutation tests, which we proved to be effective in the context of the paper. Such tests help focusing the analysis not on the assumptions, but on the actual anomalies present in the data.</p>
<p>At the end of our review, we do believe that there is a significant evidence that RTS has suspicious data. However, we recommend the authors to collect additional information since some of our tests suggest that other investigator’s data have anomalies as well, if we do not discount the huge fraction of data given by RTS.</p>
<h1 id="sec:acknowledgments" class="unnumbered">Acknowledgments</h1>
<p>We would like to thank the authors H. Pitt and H. Hill for publishing in an open journal, and making the data available for everyone. Also, we would like to thank Prof Philip Stark for his valuable and critical guidelines and timely feedback. We would also like to thank Yuansi Chen for valuable tips with python. As a final note, we would like to claim complete responsibility for all the opinions expressed in this paper.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>abuse of terminology, used in place of normalized histograms<a href="#fnref1">↩</a></p></li>
</ol>
</div>
