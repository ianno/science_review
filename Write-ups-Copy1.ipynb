{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review of paper \n",
    "\n",
    "1. Introduction\n",
    "\n",
    "2. Summary\n",
    "\n",
    "3. Replication\n",
    "\n",
    "4. Beyond the paper\n",
    "\n",
    "5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "This paper reviews some necessary considerations of the published paper \"Statistical analysis of numerical preclinical radiobiological data\". Scientific fraud and data manipulation have become more and more serious problems for scientific researchers. Statistical models to identigy potential data fabrication needs to be utilized more, which could help to determine data fabrication and falsification, suspected fraud, duplicate publication, and plagiarism [Ref_1]. Specifically for radiobiological experiment results in data sets consists of triplicate counts where the means of the triples are the key values in subsequent analyses. Potentially, researchers would be likely to manipulate the data to guide the results of such experiments, so that the average values could be within a desired region. And thus, in this manner, mid-ratio ( the ratio of the difference between the middle value and the smallest value to the difference between the largest value and the smallest value was close to 0.5 or an unusually large number of triples that actually include their own (rounded) mean as one of their values) [Ref_2]. The least significant (rightmost) digits of radiobiological data have been expected to follow a uniform distribution, which could be applied to detect if there is any fraud in the data provided by other researchers, when the least significant digit does not follow that behavior. Similar expections of equal digits of non-important digit could also applied to detect fraud and manuulay manipulated data by data. By doing that, researchers made several assumptions, such as a Poisson distribution model was used to probabilistically correspond the number of cells from tripicates, which were validated by data sets from other researchers.  \n",
    "\n",
    "In this review paper, we replicated the statistical experiments conducted by these researchers to have a better understanding of their philosophy to determine data frauds. In addition, the underlying assumptions were checked by us to show if that subjectively introduced assumptions's effects on the outcome of the test statistics. A cross-validation process was also applied to check the difference between the grouped data and individual data sets. Hypothesis testings were also applied by using different models to show the effectiveness of the models that were used by researchers to detect data fraud. By way of doing this, we are able to test the effectiveness of the statistical experiments provided by these researchers, and also provide additional reasonable assumptions, which is much closer to real life situations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Replication\n",
    "\n",
    "In this section, we replicated the statistical experiments that were conducted by the researchers. It follows an order of: 3.1 Mid-Ratio Analysis; 3.2 Hypothesis Testing I - a nonparametric test; 3.3 Hypothesis testing II - using lambda to obtain p-values; 3.4 Hypothesis testing III - normal estimation of p-values; 3.5 Terminal digit analysis; 3.6 Equal digit analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Mid-Ratio Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is to for your guys to paste code and results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Hypothesis Testing I - a nonparametric test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this section, we tried to replicate what they did in the paper. A slight difference of the final probabilities were observed in our calculations compared to their results, where the turning point of lambda value is around 11, which means the probability keeps increasing to its maximum value till lambda equals to 11, and then it decreases as lambda increases. However, we got a similar result for the threhold value for probability. In the paper, they got 0.42, and we got 0.433. As their details calculations are not included in this paper, so we completed the statistical calculations based on their assumptions, such as the triple is generated by independent, identical Poisson variables with known parameter lambda includes its own (rounded) mean value. We applied their assumptions of iid variables for the counts of micro organisms, and got our results. However, their assumptions may not always hold at different situations, thus needs further consideration.\n",
    "\n",
    "As the data is acquired by different researchers in the biological lab, a lot of biases would be introduced, such as the skillness of the researcher to take samples, how fluent they are at this specific task, different growth situations for micro organisms, how accurate their instruments are. If we have those biases present in our data, our assumptions, such as i.i.d variables would not be valiaded, and thus our corresponding calculations will not be accurate enough. Thus based on this logic, their reasoning about the difference between the RTS data and outside lab researcher data analysis should probably be on held. \n",
    "\n",
    "A detailed calculation is in the Appendix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 Hypothesis testing II - using lambda to obtain p-values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this section, the researchers from the paper used the lambda values to calculate the corresponding p-value. They applied a heuristic method to estimate the actual probability that a given collection of n triples includes k mean containing tripes to legitimate experimental data, and such that they are able to confirm the validity of their models, which is the Poission model. As the true lambda value of the Poisson variables that generated the triples in the datasets are unknown, they took advantage of the lambda MidProb table to estimate the true value, based on the fact that mean of any actural triple is a resonable estimate of the lambda parameter of the variables. In addition, a Poisson binomial distribution is assigned to Poisson binominal random variables, which is the case in their paper. From the characteristics of a poisson binomial variables, the mean is the sum of all p values, and the standard deviation is the sum of all p*(1-p) values, which could be used for further corresponding hypothesis tests. And thus they used this idea to test the RTS collection of 1343 colony trples, and came to a conclusion that the probability is an extremely small number, which contradicted same test results for other investigators. \n",
    "\n",
    "Several perspectives should be considered when conducting these statistical tests. We need to check the underlying assumptions such as a Poisson bernouli variables. One thing to note is that those data has underlying microbial phenomenna behind them, so when we just treat them as a number, then we would possibly lose a lot of intrinisic characteristics of the data. From this logic, the assumption that the triplets follows a poisson distribution will also be argued, as the sample would be taken from different growth stage of the organisms, biases would be introduced to devalidate the Possion assumption, which lead to the faliure of a Poisson binomial variable assumption. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Hypothesis testing III - normal estimation of p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 Terminal digit analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.6 Equal digit Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Beyond the paper\n",
    "\n",
    "In this section, after we replicated the statistical experiments from the researchers, we will double check their underlying assumptions, such as poission model, normal distribution, and so on. Besides, we also used cross-validation to show if the combined data mid-ratio would give us a different results if we take each data set individually and calculate corresponding mid-ratios. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Permutation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Discussions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 Results interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data fraud is an extremely important topic in science, enginering and many other subjects. Methods to detect the manually manipulated data are needed to identy the existance of data fraud, data fabrication and falsification. In our review, we conducted permutation test, blah blah tests to deterine if there is any underlying data fabrication and falisicaiton of this paper. \n",
    "\n",
    "From permutation test, we found...\n",
    "\n",
    "From ... test, we found that...\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
